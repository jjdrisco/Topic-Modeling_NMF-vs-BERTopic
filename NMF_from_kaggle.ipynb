{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>journal_issn</th>\n",
       "      <th>language</th>\n",
       "      <th>abstract</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmid</th>\n",
       "      <th>citation_count</th>\n",
       "      <th>IOSPressVolume</th>\n",
       "      <th>publication_type</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>topics</th>\n",
       "      <th>affiliation_countries</th>\n",
       "      <th>affiliations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Using an open source observational tool to mea...</td>\n",
       "      <td>2009</td>\n",
       "      <td>0926-9630</td>\n",
       "      <td>eng</td>\n",
       "      <td>Computerization of general practice is an inte...</td>\n",
       "      <td>None</td>\n",
       "      <td>19745467</td>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>[Journal Article]</td>\n",
       "      <td>[De Lusignan, S, Kumarapeli, P, Debar, S, Kush...</td>\n",
       "      <td>[Attitude to Computers, Computer Systems, Deci...</td>\n",
       "      <td>[EPR systems, consultation, primary care compu...</td>\n",
       "      <td>[united kingdom]</td>\n",
       "      <td>[St George's University of London, London SW17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Portable devices, sensors and networks: wirele...</td>\n",
       "      <td>2009</td>\n",
       "      <td>0926-9630</td>\n",
       "      <td>eng</td>\n",
       "      <td>The 21st century healthcare systems aim at inv...</td>\n",
       "      <td>None</td>\n",
       "      <td>19745466</td>\n",
       "      <td>7</td>\n",
       "      <td>150</td>\n",
       "      <td>[Journal Article, Research Support, Non-U.S. G...</td>\n",
       "      <td>[Pharow, P, Blobel, B, Ruotsalainen, P, Peters...</td>\n",
       "      <td>[Health Services, Humans, Internet, Precision ...</td>\n",
       "      <td>[health services, Portable devices, portable d...</td>\n",
       "      <td>[germany]</td>\n",
       "      <td>[eHealth Competence Center, Regensburg Univers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Archetype-based knowledge management for seman...</td>\n",
       "      <td>2009</td>\n",
       "      <td>0926-9630</td>\n",
       "      <td>eng</td>\n",
       "      <td>Formal modeling of clinical content that can b...</td>\n",
       "      <td>None</td>\n",
       "      <td>19745465</td>\n",
       "      <td>13</td>\n",
       "      <td>150</td>\n",
       "      <td>[Journal Article]</td>\n",
       "      <td>[Garde, S, Chen, R, Leslie, H, Beale, T, McNic...</td>\n",
       "      <td>[Medical Record Linkage, Medical Records Syste...</td>\n",
       "      <td>[compliance templates, templates, archetype re...</td>\n",
       "      <td>[united kingdom]</td>\n",
       "      <td>[Ocean Informatics, London, UK. sebastian.gard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is there a common background to support better...</td>\n",
       "      <td>2009</td>\n",
       "      <td>0926-9630</td>\n",
       "      <td>eng</td>\n",
       "      <td>The workshop is proposed by the EFMI WG Health...</td>\n",
       "      <td>None</td>\n",
       "      <td>19745464</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>[Journal Article]</td>\n",
       "      <td>[Stoicu-Tivadar, L, Blobel, B, Kern, J, Masic,...</td>\n",
       "      <td>[Education, Europe, Humans, International Coop...</td>\n",
       "      <td>[healthcare services, better healthcare, healt...</td>\n",
       "      <td>[romania]</td>\n",
       "      <td>[University Politehnica Timisoara, Romania. st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Digital pathology in Europe: coordinating pati...</td>\n",
       "      <td>2009</td>\n",
       "      <td>0926-9630</td>\n",
       "      <td>eng</td>\n",
       "      <td>The COST Action IC0604  Telepathology Network ...</td>\n",
       "      <td>None</td>\n",
       "      <td>19745463</td>\n",
       "      <td>7</td>\n",
       "      <td>150</td>\n",
       "      <td>[Journal Article]</td>\n",
       "      <td>[Garcia Rojo, M, Punys, V, Slodkowska, J, Schr...</td>\n",
       "      <td>[Biomedical Research, Europe, Humans, Medical ...</td>\n",
       "      <td>[research efforts, Anatomic Pathology, Patholo...</td>\n",
       "      <td>[spain]</td>\n",
       "      <td>[Hospital General de Ciudad Real, 13005 Ciudad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  year journal_issn  \\\n",
       "0  Using an open source observational tool to mea...  2009    0926-9630   \n",
       "1  Portable devices, sensors and networks: wirele...  2009    0926-9630   \n",
       "2  Archetype-based knowledge management for seman...  2009    0926-9630   \n",
       "3  Is there a common background to support better...  2009    0926-9630   \n",
       "4  Digital pathology in Europe: coordinating pati...  2009    0926-9630   \n",
       "\n",
       "  language                                           abstract   doi      pmid  \\\n",
       "0      eng  Computerization of general practice is an inte...  None  19745467   \n",
       "1      eng  The 21st century healthcare systems aim at inv...  None  19745466   \n",
       "2      eng  Formal modeling of clinical content that can b...  None  19745465   \n",
       "3      eng  The workshop is proposed by the EFMI WG Health...  None  19745464   \n",
       "4      eng  The COST Action IC0604  Telepathology Network ...  None  19745463   \n",
       "\n",
       "   citation_count  IOSPressVolume  \\\n",
       "0               4             150   \n",
       "1               7             150   \n",
       "2              13             150   \n",
       "3               0             150   \n",
       "4               7             150   \n",
       "\n",
       "                                    publication_type  \\\n",
       "0                                  [Journal Article]   \n",
       "1  [Journal Article, Research Support, Non-U.S. G...   \n",
       "2                                  [Journal Article]   \n",
       "3                                  [Journal Article]   \n",
       "4                                  [Journal Article]   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [De Lusignan, S, Kumarapeli, P, Debar, S, Kush...   \n",
       "1  [Pharow, P, Blobel, B, Ruotsalainen, P, Peters...   \n",
       "2  [Garde, S, Chen, R, Leslie, H, Beale, T, McNic...   \n",
       "3  [Stoicu-Tivadar, L, Blobel, B, Kern, J, Masic,...   \n",
       "4  [Garcia Rojo, M, Punys, V, Slodkowska, J, Schr...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  [Attitude to Computers, Computer Systems, Deci...   \n",
       "1  [Health Services, Humans, Internet, Precision ...   \n",
       "2  [Medical Record Linkage, Medical Records Syste...   \n",
       "3  [Education, Europe, Humans, International Coop...   \n",
       "4  [Biomedical Research, Europe, Humans, Medical ...   \n",
       "\n",
       "                                              topics affiliation_countries  \\\n",
       "0  [EPR systems, consultation, primary care compu...      [united kingdom]   \n",
       "1  [health services, Portable devices, portable d...             [germany]   \n",
       "2  [compliance templates, templates, archetype re...      [united kingdom]   \n",
       "3  [healthcare services, better healthcare, healt...             [romania]   \n",
       "4  [research efforts, Anatomic Pathology, Patholo...               [spain]   \n",
       "\n",
       "                                        affiliations  \n",
       "0  [St George's University of London, London SW17...  \n",
       "1  [eHealth Competence Center, Regensburg Univers...  \n",
       "2  [Ocean Informatics, London, UK. sebastian.gard...  \n",
       "3  [University Politehnica Timisoara, Romania. st...  \n",
       "4  [Hospital General de Ciudad Real, 13005 Ciudad...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's take a look at our dataset\n",
    "documents = pd.read_json(\"dataset-mie.json\")\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "language\n",
       "eng    4565\n",
       "ger      41\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore dataset for different language types\n",
    "documents[\"language\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pmid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19745467</th>\n",
       "      <td>Computerization of general practice is an inte...</td>\n",
       "      <td>[Attitude to Computers, Computer Systems, Deci...</td>\n",
       "      <td>[EPR systems, consultation, primary care compu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19745466</th>\n",
       "      <td>The 21st century healthcare systems aim at inv...</td>\n",
       "      <td>[Health Services, Humans, Internet, Precision ...</td>\n",
       "      <td>[health services, Portable devices, portable d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19745465</th>\n",
       "      <td>Formal modeling of clinical content that can b...</td>\n",
       "      <td>[Medical Record Linkage, Medical Records Syste...</td>\n",
       "      <td>[compliance templates, templates, archetype re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19745464</th>\n",
       "      <td>The workshop is proposed by the EFMI WG Health...</td>\n",
       "      <td>[Education, Europe, Humans, International Coop...</td>\n",
       "      <td>[healthcare services, better healthcare, healt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19745463</th>\n",
       "      <td>The COST Action IC0604  Telepathology Network ...</td>\n",
       "      <td>[Biomedical Research, Europe, Humans, Medical ...</td>\n",
       "      <td>[research efforts, Anatomic Pathology, Patholo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   abstract  \\\n",
       "pmid                                                          \n",
       "19745467  Computerization of general practice is an inte...   \n",
       "19745466  The 21st century healthcare systems aim at inv...   \n",
       "19745465  Formal modeling of clinical content that can b...   \n",
       "19745464  The workshop is proposed by the EFMI WG Health...   \n",
       "19745463  The COST Action IC0604  Telepathology Network ...   \n",
       "\n",
       "                                                   keywords  \\\n",
       "pmid                                                          \n",
       "19745467  [Attitude to Computers, Computer Systems, Deci...   \n",
       "19745466  [Health Services, Humans, Internet, Precision ...   \n",
       "19745465  [Medical Record Linkage, Medical Records Syste...   \n",
       "19745464  [Education, Europe, Humans, International Coop...   \n",
       "19745463  [Biomedical Research, Europe, Humans, Medical ...   \n",
       "\n",
       "                                                     topics  \n",
       "pmid                                                         \n",
       "19745467  [EPR systems, consultation, primary care compu...  \n",
       "19745466  [health services, Portable devices, portable d...  \n",
       "19745465  [compliance templates, templates, archetype re...  \n",
       "19745464  [healthcare services, better healthcare, healt...  \n",
       "19745463  [research efforts, Anatomic Pathology, Patholo...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only look at english doucments\n",
    "documents = documents[documents['language'] == 'eng'][['abstract', 'keywords', 'topics', 'pmid']]\n",
    "documents = documents.set_index('pmid')\n",
    "documents = documents[len(documents['abstract']) > 0]\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4497"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the documents that are empty\n",
    "documents = documents[documents[\"abstract\"] != '']\n",
    "documents.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "881.7986856516977"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_per_document = documents[\"abstract\"].apply(len)\n",
    "words_per_document.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import string\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/johndriscoll/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/johndriscoll/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classifi', 'classifi', 'classifi']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[SnowballStemmer('english').stem(word) for word in ['classify', 'classifier', 'classifying']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['will', 'not']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return c_dict[match.group(0)]\n",
    "    return c_re.sub(replace, text).split(\" \")\n",
    "\n",
    "expandContractions(\"won't\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'has' in stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize\n",
    "\n",
    "# Contraction map\n",
    "c_dict = {\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"I would\",\n",
    "    \"i'd've\": \"I would have\",\n",
    "    \"i'll\": \"I will\",\n",
    "    \"i'll've\": \"I will have\",\n",
    "    \"i'm\": \"I am\",\n",
    "    \"i've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there had\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we had\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'alls\": \"you alls\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you had\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you you will\",\n",
    "    \"you'll've\": \"you you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "# Compiling the contraction dict\n",
    "c_re = re.compile('(%s)' % '|'.join(c_dict.keys()))\n",
    "\n",
    "# List of stop words\n",
    "add_stop = ['said', 'say', '...', 'like']\n",
    "stop_words = ENGLISH_STOP_WORDS.union(add_stop).union(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# List of punctuation\n",
    "punc = list(set(string.punctuation))\n",
    "\n",
    "\n",
    "# Splits words on white spaces (leaves contractions intact) and splits out\n",
    "# trailing punctuation\n",
    "def casual_tokenizer(text):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return c_dict[match.group(0)]\n",
    "    return c_re.sub(replace, text)#.split(\" \")\n",
    "\n",
    "def strip_punc(word):\n",
    "        for c in punc:\n",
    "            word = word.replace(c, \"\")\n",
    "        return word\n",
    "\n",
    "def process_text(text):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    text = casual_tokenizer(text)\n",
    "    text = [each.lower() for each in text]\n",
    "    #text = [re.sub('[0-9]+', '', each) for each in text] #removes all numbers from words\n",
    "    text = [each for each in text if not any(c.isdigit() for c in each)] #removes all words with numbers\n",
    "    \"\"\"We usually remove numbers when we do text clustering or getting keyphrases as we numbers doesn’t give much importance to get the main words. \"\"\"\n",
    "    text = [expandContractions(each, c_re=c_re) for each in text]\n",
    "    # temp = []\n",
    "    # for each in text:\n",
    "    #     temp += expandContractions(each, c_re=c_re)\n",
    "    # text = temp\n",
    "    #have to remove stopwords before lemmatizing them \n",
    "    text = [w for w in text if w not in stop_words]\n",
    "    text = [wnl.lemmatize(each) if each else each for each in text]\n",
    "    text = [w for w in text if w not in punc]\n",
    "    text = [strip_punc(w) for w in text]\n",
    "    text = [w for w in text if w not in stop_words]\n",
    "    text = [each for each in text if len(each) > 1]\n",
    "    text = [each for each in text if ' ' not in each]\n",
    "    return text\n",
    "\n",
    "\n",
    "def top_words(topic, n_top_words):\n",
    "    return topic.argsort()[:-n_top_words - 1:-1]  \n",
    "\n",
    "\n",
    "def topic_table(model, feature_names, n_top_words):\n",
    "    topics = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        t = (topic_idx)\n",
    "        topics[t] = [feature_names[i] for i in top_words(topic, n_top_words)]\n",
    "    return pd.DataFrame(topics)\n",
    "\n",
    "\n",
    "def whitespace_tokenizer(text): \n",
    "    pattern = r\"(?u)\\b\\w\\w+\\b\" \n",
    "    tokenizer_regex = RegexpTokenizer(pattern)\n",
    "    tokens = tokenizer_regex.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Funtion to remove duplicate words\n",
    "def unique_words(text): \n",
    "    ulist = []\n",
    "    [ulist.append(x) for x in text if x not in ulist]\n",
    "    return ulist\n",
    "\n",
    "\n",
    "def word_count(text):\n",
    "    return len(str(text).split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pmid\n",
       "19745467    [computerization, general, practice, internati...\n",
       "19745466    [century, healthcare, aim, involving, citizen,...\n",
       "19745465    [formal, modeling, clinical, content, availabl...\n",
       "19745464    [workshop, proposed, efmi, wg, health, informa...\n",
       "19745463    [cost, action, telepathology, network, europe,...\n",
       "                                  ...                        \n",
       "39176482    [study, advance, utility, synthetic, study, da...\n",
       "39176481    [administrable, dose, form, obtained, transfor...\n",
       "39176480    [key, research, area, kras, identified, establ...\n",
       "39176479    [paper, present, versatile, solution, formally...\n",
       "39176478    [international, classification, icd, icd, sexs...\n",
       "Name: processed_abstracts, Length: 4497, dtype: object"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### tokenize the abstracts\n",
    "documents['processed_abstracts'] = documents['abstract'].apply(process_text)\n",
    "documents['processed_abstracts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data</td>\n",
       "      <td>5334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>patient</td>\n",
       "      <td>3944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>health</td>\n",
       "      <td>3506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>information</td>\n",
       "      <td>3211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clinical</td>\n",
       "      <td>2455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>medical</td>\n",
       "      <td>2383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>study</td>\n",
       "      <td>2352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>care</td>\n",
       "      <td>2063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>model</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>result</td>\n",
       "      <td>1791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>paper</td>\n",
       "      <td>1791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>used</td>\n",
       "      <td>1683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>method</td>\n",
       "      <td>1647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>use</td>\n",
       "      <td>1631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>using</td>\n",
       "      <td>1588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>healthcare</td>\n",
       "      <td>1534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>based</td>\n",
       "      <td>1472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>process</td>\n",
       "      <td>1460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>research</td>\n",
       "      <td>1339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>approach</td>\n",
       "      <td>1291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  frequency\n",
       "0          data       5334\n",
       "1       patient       3944\n",
       "2        health       3506\n",
       "3   information       3211\n",
       "4      clinical       2455\n",
       "5       medical       2383\n",
       "6         study       2352\n",
       "7          care       2063\n",
       "8         model       2000\n",
       "9        result       1791\n",
       "10        paper       1791\n",
       "11         used       1683\n",
       "12       method       1647\n",
       "13          use       1631\n",
       "14        using       1588\n",
       "15   healthcare       1534\n",
       "16        based       1472\n",
       "17      process       1460\n",
       "18     research       1339\n",
       "19     approach       1291"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the top 20 most common words among all the articles\n",
    "p_text = documents['processed_abstracts']\n",
    "\n",
    "# Flaten the list of lists\n",
    "p_text = [item for sublist in p_text for item in sublist]\n",
    "\n",
    "# Top 20\n",
    "top_20 = pd.DataFrame(\n",
    "    Counter(p_text).most_common(20),\n",
    "    columns=['word', 'frequency']\n",
    ")\n",
    "\n",
    "top_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08695652173913043"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1600/18400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['aa', 'aaa', 'aact', ..., 'österle', 'αu', 'βblockers'],\n",
       "       dtype='<U60'),\n",
       " 18400)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(p_text), len(np.unique(p_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    min_df=3,\n",
    "    max_df=0.85,\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    preprocessor=' '.join\n",
    ")\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
